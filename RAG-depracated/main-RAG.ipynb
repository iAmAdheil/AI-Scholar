{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc463f74-fa4d-4cb1-bff4-15a392325942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Contents lists available at ScienceDirect\n",
      "Parallel Computing\n",
      "journal homepage: www.elsevier.com/locate/parco  \n",
      "Software acceleration of multi-user MIMO uplink detection on GPU\n",
      "Ali Nada a\n",
      " ,∗, Hazem Is\n",
      "\n",
      "{'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'Elsevier', 'creationdate': '2025-09-05T09:12:22+00:00', 'crossmarkdomains[1]': 'elsevier.com', 'crossmarkmajorversiondate': '2010-04-23', 'creationdate--text': '5th September 2025', 'elsevierwebpdfspecifications': '7.0.1', 'robots': 'noindex', 'moddate': '2025-09-05T09:13:47+00:00', 'author': 'Ali Nada', 'doi': '10.1016/j.parco.2025.103150', 'title': 'Software acceleration of multi-user MIMO uplink detection on GPU', 'keywords': 'High-performance computing,Parallel computing,Massive MIMO,Uplink detection,Matrix decomposition', 'subject': 'Parallel Computing, 125 (2025) 103150. doi:10.1016/j.parco.2025.103150', 'crossmarkdomains[2]': 'sciencedirect.com', 'crossmarkdomainexclusive': 'true', 'source': './paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# load text and metadata filters from pdf\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./paper.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79645d1a-7b0b-419b-a40d-1c7b664dd492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Software acceleration of multi-user MIMO uplink detection on GPU', 'authors': 'Ali Nada, Hazem Ismail Ali, Liang Liu, Yousra Alkabani', 'publish_year': 2025, 'venue': 'Parallel Computing', 'source': 'paper.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# normalise and extract meta data filters from pdf content\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import json\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # or \"gemini-1.5-flash\" for faster responses\n",
    "    google_api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    temperature=0.1  # Lower for more factual responses\n",
    ")\n",
    "\n",
    "context = \"\"\n",
    "for i in range(min(5, len(docs))):\n",
    "    context += docs[i].page_content + \"\\n\\n\"\n",
    "context = context[:6000]  # Safe token limit\n",
    "\n",
    "# Prompt: extract metadata from front matter only\n",
    "prompt = f\"\"\"\n",
    "    Extract metadata from this academic paper's front matter.\n",
    "    Return ONLY valid JSON with these keys. If not found, use null.    \n",
    "    \n",
    "    Keys:\n",
    "    - title: string\n",
    "    - authors: string (comma-separated)\n",
    "    - publish_year: integer (4 digits) or null\n",
    "    - venue: string (e.g., \"NeurIPS 2024\", \"arXiv\", \"Nature\")\n",
    "    \n",
    "    Content:\n",
    "    {context}\n",
    "    \n",
    "    Respond with ONLY the JSON object, no markdown formatting:\n",
    "    {{\n",
    "      \"title\": \"...\",\n",
    "      \"authors\": \"...\",\n",
    "      \"publish_year\": 2024,\n",
    "      \"venue\": \"NeurIPS 2024\"\n",
    "    }}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "filters_json = response.content\n",
    "filters = json.loads(filters_json)\n",
    "\n",
    "# normalise the year\n",
    "if filters.get(\"publish_year\"):\n",
    "    try:\n",
    "        filters[\"publish_year\"] = int(filters[\"publish_year\"])\n",
    "    except Exception:\n",
    "        filters[\"publish_year\"] = None\n",
    "filters[\"source\"] = \"paper.pdf\"\n",
    "\n",
    "# # authors as an array of strings\n",
    "# raw_authors = filters.get(\"authors\", \"\")\n",
    "# if raw_authors and isinstance(raw_authors, str):\n",
    "#     # Split by comma, strip whitespace, remove empty\n",
    "#     filters[\"authors\"] = [\n",
    "#         author.strip() for author in raw_authors.split(\",\")\n",
    "#         if author.strip()\n",
    "#     ]\n",
    "# else:\n",
    "#     filters[\"authors\"] = []\n",
    "\n",
    "print(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "090c6593-ea9a-419e-9dd1-4d4ebb2d16e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "Contents lists available at ScienceDirect\n",
      "Parallel Computing\n",
      "journal homepage: www.elsevier.com/locate/parco  \n",
      "Software acceleration of multi-user MIMO uplink detection on GPU\n",
      "Ali Nada a\n",
      " ,∗, Hazem Ismail Ali a, Liang Liu b, Yousra Alkabani a\n",
      "a Halmstad University, Box 823, Halmstad, 301 18, Halland, Sweden\n",
      "b Lund University, Box 117, Lund, 221 00, Skåne, Sweden\n",
      "A R T I C L E  I N F O\n",
      "Keywords:\n",
      "High-performance computing\n",
      "Parallel computing\n",
      "Massive MIMO\n",
      "Uplink detection\n",
      "Matrix decomposition\n",
      " A B S T R A C T\n",
      "This paper presents the exploration of GPU-accelerated block-wise decompositions for zero-forcing (ZF) \n",
      "based QR and Cholesky methods applied to massive multiple-input multiple-output (MIMO) uplink detection \n",
      "algorithms. Three algorithms are evaluated: ZF with block Cholesky decomposition, ZF with block QR \n",
      "decomposition (QRD), and minimum mean square error (MMSE) with block Cholesky decomposition. The latter\n",
      "{'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'Elsevier', 'creationdate': '2025-09-05T09:12:22+00:00', 'crossmarkdomains[1]': 'elsevier.com', 'crossmarkmajorversiondate': '2010-04-23', 'creationdate--text': '5th September 2025', 'elsevierwebpdfspecifications': '7.0.1', 'robots': 'noindex', 'moddate': '2025-09-05T09:13:47+00:00', 'author': 'Ali Nada', 'doi': '10.1016/j.parco.2025.103150', 'title': 'Software acceleration of multi-user MIMO uplink detection on GPU', 'keywords': 'High-performance computing,Parallel computing,Massive MIMO,Uplink detection,Matrix decomposition', 'subject': 'Parallel Computing, 125 (2025) 103150. doi:10.1016/j.parco.2025.103150', 'crossmarkdomains[2]': 'sciencedirect.com', 'crossmarkdomainexclusive': 'true', 'source': 'paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'authors': 'Ali Nada, Hazem Ismail Ali, Liang Liu, Yousra Alkabani', 'publish_year': 2025, 'venue': 'Parallel Computing'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, length_function=len\n",
    ").split_documents(docs)\n",
    "\n",
    "# 4. merge metadata\n",
    "for chunk in chunks:\n",
    "    chunk.metadata = {**chunk.metadata, **filters, \"total_pages\": len(docs)}\n",
    "\n",
    "print(len(chunks))\n",
    "print(chunks[0].page_content)\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6858139-38d4-4236-93bb-de21f7875b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=research_papers)]\n"
     ]
    }
   ],
   "source": [
    "# build knowledge base -> parse documents and embed them in the persistent vectorDB\n",
    "# how will images be handled\n",
    "\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import google.generativeai as genai\n",
    "\n",
    "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
    "    api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    model_name=\"gemini-embedding-001\" # Set the model explicitly\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "chroma_client.heartbeat()\n",
    "\n",
    "collections = chroma_client.list_collections()\n",
    "print(collections)\n",
    "\n",
    "# Create / get your collection with Google embeddings\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "\tname=\"research_papers\",\n",
    "\tembedding_function=google_ef\n",
    ")\n",
    "\n",
    "# Add your research paper chunks\n",
    "\n",
    "# Each page_content is embedded automatically using google_embedding_fn\n",
    "# Each metadata_filters dict is stored alongside its vector\n",
    "# ids uniquely identify each chunk\n",
    "\n",
    "collection.add(\n",
    "\tdocuments=[chunk.page_content for chunk in chunks],\n",
    "\tmetadatas=[chunk.metadata for chunk in chunks],\n",
    "\tids=[str(i) for i in range(len(chunks))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb2391e6-9cc6-48be-bf1d-b8547aa19649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 103\n"
     ]
    }
   ],
   "source": [
    "# verify if document has been embedded\n",
    "\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d565d29-88af-4f4f-b065-4145b55a260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"what is attention?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef7efdd-3051-49b0-923f-7790a951770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: specific-to-paper\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# classification agent\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # or \"gemini-1.5-flash\" for faster responses\n",
    "    google_api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    temperature=0.1  # Lower for more factual responses\n",
    ")\n",
    "    \n",
    "# 3. Create prompt with context + query\n",
    "prompt = f\"\"\"\n",
    "You are a Research Question Classification Agent.\n",
    "Your task is to analyze a user’s question and classify it into one of three categories based on intent and context.\n",
    "\n",
    "Classification categories:\n",
    "\n",
    " - Specific-to-paper — The question references or implies a specific research paper, author, DOI, or title (e.g., “What does the 2021 paper by Smith et al. conclude about transformers?”).\n",
    "\n",
    " - Generic-research — The question is about a research topic or domain in general, not a single paper (e.g., “How do attention mechanisms work in NLP?”).\n",
    "\n",
    " - Non-research — The question is unrelated to academic or scientific research (e.g., “What’s the weather today?” or “Write a poem.”).\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Always output only one label: specific-to-paper, generic-research, or non-research.\n",
    "\n",
    "If uncertain, choose the closest category by analyzing research intent.\n",
    "\n",
    "Do not include explanations or reasoning in the final output.\n",
    "\n",
    "Question: {user_query}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Send to Gemini\n",
    "response = llm.invoke(prompt)\n",
    "query_type = response.content # answer -> specific-to-paper / generic-research / non-research\n",
    "\n",
    "print(\"Query Type:\", query_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ea9516a-0b3d-4c9f-bbf0-42887d934e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_type.lower() == \"non-research\":\n",
    "    print(\"Irrelevant question.\")\n",
    "    # break and return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "483c27d6-2045-4db9-9518-cc9bb064c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    \"\"\"Input for academic paper search queries.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"The research topic or question to search for (e.g., 'Multi-user MIMO', 'transformer architecture')\"\n",
    "    )\n",
    "\n",
    "@tool(\"web_search\", args_schema=SearchInput, return_direct=False)\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search academic papers on Semantic Scholar for research questions.\n",
    "    Returns paper titles, abstracts, and summaries.\n",
    "    \n",
    "    Args:\n",
    "        query: Research topic to search for\n",
    "        \n",
    "    Returns:\n",
    "        Formatted context with paper information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build the API URL\n",
    "        base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "        fields = \"title,abstract,authors,year,venue,url,tldr\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"fields\": fields,\n",
    "            \"limit\": 3\n",
    "        }\n",
    "        \n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we got results\n",
    "        if not data.get('data') or len(data['data']) == 0:\n",
    "            return f\"No papers found for query: {query}\"\n",
    "        \n",
    "        # Format the context\n",
    "        context = f\"Found {data.get('total', 0)} papers on '{query}'. Top {len(data['data'])} results:\\n\\n\"\n",
    "        \n",
    "        for i, paper in enumerate(data['data'], 1):\n",
    "            authors = paper.get('authors', [])\n",
    "            author_names = ', '.join([a.get('name', 'Unknown') for a in authors[:3]])\n",
    "            if len(authors) > 3:\n",
    "                author_names += ' et al.'\n",
    "            \n",
    "            context += f\"--- Paper {i} ---\\n\"\n",
    "            context += f\"Title: {paper.get('title', 'N/A')}\\n\"\n",
    "            context += f\"Authors: {author_names}\\n\"\n",
    "            context += f\"Year: {paper.get('year', 'N/A')}\\n\"\n",
    "            context += f\"Venue: {paper.get('venue', 'N/A')}\\n\"\n",
    "            context += f\"URL: {paper.get('url', 'N/A')}\\n\"\n",
    "            context += f\"\\nAbstract:\\n{paper.get('abstract', 'Not available')}\\n\"\n",
    "            \n",
    "            tldr = paper.get('tldr')\n",
    "            if tldr and tldr.get('text'):\n",
    "                context += f\"\\nKey Summary:\\n{tldr['text']}\\n\"\n",
    "            \n",
    "            context += \"\\n\"\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out. Please try again.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching papers: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c616fb9c-136a-4689-8c08-74820ae06d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do attention mechanisms work in NLP?\n",
      "{'messages': [HumanMessage(content='How do attention mechanisms work in NLP?', additional_kwargs={}, response_metadata={}, id='bdd3a3f7-5f8f-480d-a21d-298c4754e9aa'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'web_search', 'arguments': '{\"query\": \"attention mechanisms in NLP\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--842cc8ef-f2db-430d-9e5e-f941359ce484-0', tool_calls=[{'name': 'web_search', 'args': {'query': 'attention mechanisms in NLP'}, 'id': 'de4aeae5-f2a7-4721-a339-0ca9860aa790', 'type': 'tool_call'}]), ToolMessage(content='Found 1713 papers on \\'attention mechanisms in NLP\\'. Top 3 results:\\n\\n--- Paper 1 ---\\nTitle: An Introductory Survey on Attention Mechanisms in NLP Problems\\nAuthors: Dichao Hu\\nYear: 2018\\nVenue: Intelligent Systems with Applications\\nURL: https://www.semanticscholar.org/paper/2e46eac625e70261e43fa765c22a2828e5dd2659\\n\\nAbstract:\\nFirst derived from human intuition, later adapted to machine translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this paper, we survey through recent works and conduct an introductory summary of the attention mechanism in different NLP problems, aiming to provide our readers with basic knowledge on this widely used method, discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.\\n\\nKey Summary:\\nAn introductory summary of the attention mechanism in different NLP problems is conducted, aiming to provide basic knowledge on this widely used method, to discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.\\n\\n--- Paper 2 ---\\nTitle: Enhancing COVID‐19 misinformation detection through novel attention mechanisms in NLP\\nAuthors: Anbar Hussain, Wajid Ali, Awais Ahmad et al.\\nYear: 2025\\nVenue: Expert Syst. J. Knowl. Eng.\\nURL: https://www.semanticscholar.org/paper/8ccba137d7d2d7c00e113c53f8d416f3020dc1d9\\n\\nAbstract:\\nNone\\n\\nKey Summary:\\nThe proposed model surpasses the performance of state‐of‐the‐art models in classifying fake news and achieves accuracy, F1 score, precision, and recall of 0.98, 0.96, 0.95, and 0.95, respectively.\\n\\n--- Paper 3 ---\\nTitle: Attention Mechanisms in Deep Learning : Towards Explainable Artificial Intelligence\\nAuthors: Nour El Houda Dehimi, Zakaria Tolba\\nYear: 2024\\nVenue: International Conference on Pattern Analysis and Intelligent Systems\\nURL: https://www.semanticscholar.org/paper/4a5d283647e7826a8da873c2a50c94146678165d\\n\\nAbstract:\\nAttention mechanisms have revolutionized Machine Learning (ML), particularly in Natural Language Processing (NLP). These mechanisms enable models to selectively focus on crucial parts of the input data, improving performance across tasks like machine translation and sentiment analysis. However, complex ML architectures often remain opaque. This paper explores how attention mechanisms offer a unique path towards Explainable Artificial Intelligence (XAI). By visualizing and analyzing where a model \"attends\", we can gain insights into which features or data components were most impactful for its predictions. This understanding facilitates model debugging, bias detection, and the development of more transparent AI systems. We discuss different types of attention mechanisms and their potential for explainability in various ML domains.\\n\\nKey Summary:\\nThis paper explores how attention mechanisms offer a unique path towards Explainable Artificial Intelligence (XAI) by visualizing and analyzing where a model \"attends\", and gains insights into which features or data components were most impactful for its predictions.\\n\\n', name='web_search', id='6f6fb977-21fd-4c5c-ad5c-0834d4ac98e5', tool_call_id='de4aeae5-f2a7-4721-a339-0ca9860aa790'), AIMessage(content='Attention mechanisms in NLP allow models to focus on the most relevant parts of the input data when processing it. Instead of treating all input tokens equally, attention mechanisms assign weights to different tokens, indicating their importance in the current context. This helps the model to selectively attend to the most informative parts of the input, improving performance on various NLP tasks such as machine translation, sentiment analysis, and question answering. By visualizing where a model \"attends,\" we can gain insights into which features or data components were most impactful for its predictions, facilitating model debugging, bias detection, and the development of more transparent AI systems.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--9a08bc86-765d-4219-adde-cf5dbd45f38d-0')]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "k_neighbours = 5\n",
    "\n",
    "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
    "    api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    model_name=\"gemini-embedding-001\" # Set the model explicitly\n",
    ")\n",
    "    \n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "chroma_client.heartbeat()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # or \"gemini-1.5-flash\" for faster responses\n",
    "    google_api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    temperature=0.1  # Lower for more factual responses\n",
    ")\n",
    "\n",
    "@tool\n",
    "def kb_retrieval(query: str) -> str:\n",
    "    \"\"\"Retrieve documents from the primary knowledge base.\"\"\"\n",
    "    # embed query and get k nearest neighbours -> use retrieval techniques to improve context retrieval        \n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"research_papers\",\n",
    "        embedding_function=google_ef\n",
    "    )\n",
    "    result = collection.query(\n",
    "        query_texts=[user_query],\n",
    "        n_results=k_neighbours,\n",
    "        include=[\"documents\"]\n",
    "    )\n",
    "    \n",
    "    retrieved_documents = result['documents'][0] \n",
    "    \n",
    "    # Merge the list of documents into a single string, typically separated by newlines or a delimiter.\n",
    "    # The delimiter \"---\" helps the subsequent LLM distinguish between individual documents.\n",
    "    merged_docs_string = \"\\n---\\n\".join(retrieved_documents)\n",
    "    \n",
    "    # 5. Return the Merged String\n",
    "    return merged_docs_string\n",
    "\n",
    "if query_type.lower() == \"generic-research\":\n",
    "    # retrieve from knowledge base(vector DB)\n",
    "    # check for relevance of retrieved context\n",
    "\n",
    "    # if relevent -> pass to LLM along with question and generate response\n",
    "    # else use tool get_info_from_web -> calls an API with LLM generated Url(query params) for context retrieval\n",
    "\n",
    "    # extract context and check for relevance\n",
    "    # if relevent -> pass to LLM along with question and generate response\n",
    "    # else inform user that information could not be gathered about the topic that they are searching for\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are a highly logical and systematic Research Assistant. Your sole purpose is to answer the user's query by following a strict, two-step information retrieval hierarchy. You have access to two tools: `kb_retrieval` (your internal knowledge base) and `web_search` (a public search engine).\n",
    "\n",
    "    **PROCEDURE:**\n",
    "\n",
    "    1.  **PRIMARY SOURCE:** You **MUST** first attempt to retrieve information using the `kb_retrieval` tool.\n",
    "        * **Action:** Call `kb_retrieval` with the user's exact query.\n",
    "        * **Observation Analysis:** Analyze the content returned by `kb_retrieval` (the Observation).\n",
    "        * **IF** the content is sufficient and relevant to fully answer the query, formulate your final answer using **only** that context.\n",
    "        * **IF** the content is empty, irrelevant, or clearly insufficient, you **MUST** proceed to the Secondary Source step.\n",
    "\n",
    "    2.  **SECONDARY SOURCE (Fallback):** Only if the `kb_retrieval` tool fails to provide a relevant answer, you **MUST** call the `web_search` tool.\n",
    "        * **Action:** Call `web_search` with the user's original query.\n",
    "        * **Final Answer Generation:** Use the new results from the `web_search` tool to formulate your final answer.\n",
    "        * **IF** the web search results are relevant, provide the answer and state your process is complete.\n",
    "        * **IF** the web search results are still unhelpful (e.g., \"no information found\"), you **MUST** output a `Final Answer` informing the user that information could not be gathered on this topic from your available sources.\n",
    "\n",
    "    **IMPORTANT:** You must explicitly use the tools (Action) and wait for the Observation before making a decision. Always end your process by outputting a single `Final Answer`.\n",
    "    \"\"\"\n",
    "\n",
    "    agent = create_agent(\n",
    "        model,\n",
    "        tools=[web_search, kb_retrieval]\n",
    "    )\n",
    "\n",
    "    print(user_query)\n",
    "\n",
    "    conversation = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": f\"{user_query}\"}]}\n",
    "    )\n",
    "\n",
    "    print(conversation)\n",
    "\n",
    "    # final_ai_message = conversation[-1] \n",
    "\n",
    "    # # 2. Access the 'content' field\n",
    "    # content_field = final_ai_message['content'] \n",
    "    \n",
    "    # # 3. The content field is a list of dictionaries; get the first one\n",
    "    # #    and extract the value associated with the 'text' key.\n",
    "    # final_text = content_field[0]['text']\n",
    "    \n",
    "    # print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "604f14d6-c408-4dec-861d-3e3a18aac169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "070ba815-205d-471e-99ec-0476e33ba487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, io, re, requests, PyPDF2\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\"\n",
    "\n",
    "def extract_pdf(pdf_url: str, max_pages: int = 10) -> FAISS:\n",
    "    log.info(f\"Download pdf -> {pdf_url}\")\n",
    "    try:    \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(pdf_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Create BytesIO object\n",
    "        pdf_io = io.BytesIO(response.content)\n",
    "\n",
    "        # 2. Extract per page with PyPDF2\n",
    "        reader = PyPDF2.PdfReader(pdf_io)\n",
    "        pages = reader.pages\n",
    "        total_pages = len(pages)\n",
    "        pages_to_read = min(total_pages, max_pages)\n",
    "\n",
    "        page_docs: List[Document] = []\n",
    "        for i in range(pages_to_read):\n",
    "            text = pages[i].extract_text() or \"\"\n",
    "            page_docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": pdf_url,\n",
    "                    \"page\": i,  # 0-indexed\n",
    "                    \"total_pages\": total_pages,\n",
    "                }\n",
    "            ))\n",
    "\n",
    "        # 3. Chunk\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            add_start_index=True,  # char offset in original page\n",
    "        )\n",
    "        chunks = splitter.split_documents(page_docs)\n",
    "    \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk_idx\"] = idx\n",
    "    \n",
    "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "        embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "        \n",
    "        vector_store = FAISS(\n",
    "            embedding_function=embeddings,     # How to create vectors\n",
    "            index=index,                      # Where to store vectors (needs to exist!)\n",
    "            docstore=InMemoryDocstore(),      # Where to store documents\n",
    "            index_to_docstore_id={},         # Mapping between them\n",
    "        )\n",
    "\n",
    "        vector_store.add_documents(chunks)\n",
    "\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1bb7468-8f32-41b8-b99d-d39b7fc48588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "import io\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    \"\"\"Input for academic paper search queries.\"\"\"\n",
    "    query: str = Field(\n",
    "        description=\"Based on the users query, use either complete or incomplete name of the paper or the central topic the user is referring to in their question. Higher priority to the paper's name(incomplete names are acceptable as well).\"\n",
    "    )\n",
    "\n",
    "@tool(\"specific_web_search\", args_schema=SearchInput, return_direct=False)\n",
    "def specific_web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for a paper using Semantic Scholar.\n",
    "    - If openAccessPdf available → embed & retrieve relevant chunks using user_query\n",
    "    - Else → return abstract + TL;DR\n",
    "    \n",
    "    Args:\n",
    "        query: Research topic to search for\n",
    "        \n",
    "    Returns:\n",
    "        Formatted context with paper information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        context = \"\"\n",
    "        \n",
    "        base_url = \"https://api.semanticscholar.org/graph/v1/paper/autocomplete\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        matches = response.json()\n",
    "\n",
    "        if not matches.get('matches') or len(matches['matches']) == 0:\n",
    "            return f\"Could not find paper\"\n",
    "\n",
    "        closest_matching_paper = matches['matches'][0]\n",
    "        search_id = closest_matching_paper['id']\n",
    "        \n",
    "        # Build the API URL\n",
    "        base_url = f\"https://api.semanticscholar.org/graph/v1/paper/{search_id}\"\n",
    "        fields = \"title,abstract,authors,year,venue,openAccessPdf,tldr\"\n",
    "        params = {\n",
    "            \"fields\": fields,\n",
    "        }\n",
    "        \n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        paper = response.json()\n",
    "        \n",
    "        # Check if we got results\n",
    "        if paper.get('error') and len(paper['error']) > 0:\n",
    "            return f\"No papers found for id: {search_id}\"\n",
    "            \n",
    "        if paper.get('openAccessPdf') and len(paper.get('openAccessPdf')['url']) > 0:\n",
    "            log.info(f\"URL from openAccessPdf -> {paper.get('openAccessPdf')['url']}\")\n",
    "            vector_store = extract_pdf(paper.get('openAccessPdf')['url'])\n",
    "            if vector_store:\n",
    "                # retrieve using user query and send context\n",
    "                # else format paper details and return as context\n",
    "                results = vector_store.similarity_search(\n",
    "                    user_query,\n",
    "                    k=5\n",
    "                )\n",
    "\n",
    "                context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "                return context\n",
    "                \n",
    "        authors = paper.get('authors', [])\n",
    "        author_names = ', '.join([a.get('name', 'Unknown') for a in authors[:3]])\n",
    "        if len(authors) > 3:\n",
    "            author_names += ' et al.'\n",
    "        \n",
    "        context += f\"--- Paper ---\\n\"\n",
    "        context += f\"Title: {paper.get('title', 'N/A')}\\n\"\n",
    "        context += f\"Authors: {author_names}\\n\"\n",
    "        context += f\"Year: {paper.get('year', 'N/A')}\\n\"\n",
    "        context += f\"Venue: {paper.get('venue', 'N/A')}\\n\"\n",
    "        context += f\"URL: {paper.get('openAccessPdf', 'N/A')['url']}\\n\"\n",
    "        context += f\"\\nAbstract:\\n{paper.get('abstract', 'Not available')}\\n\"\n",
    "        \n",
    "        tldr = paper.get('tldr')\n",
    "        if tldr and tldr.get('text'):\n",
    "            context += f\"\\nKey Summary:\\n{tldr['text']}\\n\"\n",
    "        \n",
    "        context += \"\\n\"\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out. Please try again.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching papers: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f768b013-817d-4178-b9a4-a0097ff13b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-26 12:44:41,817 | INFO     | chromadb.telemetry.product.posthog | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-10-26 12:44:41,878 | INFO     | httpx | HTTP Request: GET http://localhost:8000/api/v2/auth/identity \"HTTP/1.1 200 OK\"\n",
      "2025-10-26 12:44:41,879 | INFO     | chromadb.telemetry.product.posthog | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-10-26 12:44:41,898 | INFO     | httpx | HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "2025-10-26 12:44:41,901 | INFO     | httpx | HTTP Request: GET http://localhost:8000/api/v2/tenants/default_tenant/databases/default_database \"HTTP/1.1 200 OK\"\n",
      "2025-10-26 12:44:41,906 | INFO     | httpx | HTTP Request: GET http://localhost:8000/api/v2/heartbeat \"HTTP/1.1 200 OK\"\n",
      "The paper 'Attention is all', can u explain what it is about?\n",
      "{'messages': [HumanMessage(content=\"The paper 'Attention is all', can u explain what it is about?\", additional_kwargs={}, response_metadata={}, id='b590aca4-7c5a-4247-8f2b-751c6b6906e6'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'specific_web_search', 'arguments': '{\"query\": \"Attention is all you need\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--7d395295-0fb6-4dec-a6b9-848ffb435fc5-0', tool_calls=[{'name': 'specific_web_search', 'args': {'query': 'Attention is all you need'}, 'id': '1520878a-adb8-4227-ab9a-05ef0745c8c3', 'type': 'tool_call'}]), ToolMessage(content='--- Paper ---\\nTitle: Attention is All you Need\\nAuthors: Ashish Vaswani, Noam M. Shazeer, Niki Parmar et al.\\nYear: 2017\\nVenue: Neural Information Processing Systems\\nURL: \\n\\nAbstract:\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\nKey Summary:\\nA new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n', name='specific_web_search', id='27736ddd-01b7-4d07-924f-f0d1479fccf3', tool_call_id='1520878a-adb8-4227-ab9a-05ef0745c8c3'), AIMessage(content='The paper \"Attention is All You Need\" introduces the Transformer, a novel neural network architecture based solely on attention mechanisms. It moves away from recurrent and convolutional neural networks, which were the dominant sequence transduction models at the time. The Transformer demonstrates superior quality, better parallelization, and faster training times on machine translation tasks. It achieves state-of-the-art results on English-to-German and English-to-French translation tasks and generalizes well to English constituency parsing. In essence, the paper proposes a new and more efficient approach to sequence transduction using attention mechanisms.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--dc4cc372-6867-4752-949a-c7f579af0d0f-0')]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "k_neighbours = 5\n",
    "\n",
    "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
    "    api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    model_name=\"gemini-embedding-001\" # Set the model explicitly\n",
    ")\n",
    "    \n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "chroma_client.heartbeat()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # or \"gemini-1.5-flash\" for faster responses\n",
    "    google_api_key=\"AIzaSyC-rE0Ggpz0AlNeYVC3aoJXBmz2j2YS9eI\",\n",
    "    temperature=0.1  # Lower for more factual responses\n",
    ")\n",
    "\n",
    "@tool\n",
    "def kb_retrieval(query: str) -> str:\n",
    "    \"\"\"Retrieve documents from the primary knowledge base.\"\"\"\n",
    "    # embed query and get k nearest neighbours -> use retrieval techniques to improve context retrieval        \n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"research_papers\",\n",
    "        embedding_function=google_ef\n",
    "    )\n",
    "    result = collection.query(\n",
    "        query_texts=[user_query],\n",
    "        n_results=k_neighbours,\n",
    "        include=[\"documents\"]\n",
    "    )\n",
    "    \n",
    "    retrieved_documents = result['documents'][0]\n",
    "    \n",
    "    # Merge the list of documents into a single string, typically separated by newlines or a delimiter.\n",
    "    # The delimiter \"---\" helps the subsequent LLM distinguish between individual documents.\n",
    "    merged_docs_string = \"\\n---\\n\".join(retrieved_documents)\n",
    "    \n",
    "    # 5. Return the Merged String\n",
    "    return merged_docs_string\n",
    "\n",
    "if query_type.lower() == \"specific-to-paper\":\n",
    "    # retrieve from knowledge base(vector DB)\n",
    "    # check for relevance of retrieved context\n",
    "\n",
    "    # if relevent -> pass to LLM along with question and generate response\n",
    "    # else use tool get_info_from_web -> calls an API with LLM generated Url(query params) for context retrieval\n",
    "\n",
    "    # extract context and check for relevance\n",
    "    # if relevent -> pass to LLM along with question and generate response\n",
    "    # else inform user that information could not be gathered about the topic that they are searching for\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are a highly logical and systematic Research Assistant. Your sole purpose is to answer the user's query by following a strict, two-step information retrieval hierarchy. You have access to two tools: `kb_retrieval` (your internal knowledge base) and `specific_web_search` (a public search engine).\n",
    "\n",
    "    **PROCEDURE:**\n",
    "\n",
    "    1.  **PRIMARY SOURCE:** You **MUST** first attempt to retrieve information using the `kb_retrieval` tool.\n",
    "        * **Action:** Call `kb_retrieval` with the user's exact query.\n",
    "        * **Observation Analysis:** Analyze the content returned by `kb_retrieval` (the Observation).\n",
    "        * **IF** the content is sufficient and relevant to fully answer the query, formulate your final answer using **only** that context.\n",
    "        * **IF** the content is empty, irrelevant, or clearly insufficient, you **MUST** proceed to the Secondary Source step.\n",
    "\n",
    "    2.  **SECONDARY SOURCE (Fallback):** Only if the `kb_retrieval` tool fails to provide a relevant answer, you **MUST** call the `specific_web_search` tool.\n",
    "        * **Action:** Call `specific_web_search`, following instructions mentioned in tool definition.\n",
    "        * **Final Answer Generation:** Use the new results from the `specific_web_search` tool to formulate your final answer.\n",
    "        * **IF** the web search results are relevant, provide the answer and state your process is complete.\n",
    "        * **IF** the web search results are still unhelpful (e.g., \"no information found\"), you **MUST** output a `Final Answer` informing the user that information could not be gathered on this topic from your available sources.\n",
    "\n",
    "    **IMPORTANT:** You must explicitly use the tools (Action) and wait for the Observation before making a decision. Always end your process by outputting a single `Final Answer`.\n",
    "    \"\"\"\n",
    "\n",
    "    agent = create_agent(\n",
    "        model,\n",
    "        tools=[specific_web_search, kb_retrieval]\n",
    "    )\n",
    "\n",
    "    print(user_query)\n",
    "\n",
    "    conversation = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": f\"{user_query}\"}]}\n",
    "    )\n",
    "\n",
    "    print(conversation)\n",
    "\n",
    "    # final_ai_message = conversation[-1] \n",
    "\n",
    "    # # 2. Access the 'content' field\n",
    "    # content_field = final_ai_message['content'] \n",
    "    \n",
    "    # # 3. The content field is a list of dictionaries; get the first one\n",
    "    # #    and extract the value associated with the 'text' key.\n",
    "    # final_text = content_field[0]['text']\n",
    "    \n",
    "    # print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d745a-1560-42c9-b387-435b6f244bed",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "1. Take a look at why pdf extraction fails using logs when adding RAG to fastAPI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3057f2d-8f8d-4df0-8b4c-cc87c2a968c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
